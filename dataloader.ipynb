{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e9c29a3e",
      "metadata": {
        "id": "e9c29a3e"
      },
      "source": [
        "Import bibliotek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kmyjlSQ03oHO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmyjlSQ03oHO",
        "outputId": "66c6dba2-9d92-4969-e36f-ba04be5f3167"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    print(\"Jestem w Google Colab! Wykonuję specyficzny kod...\")\n",
        "    !pip install pandas\n",
        "    !pip install torch\n",
        "    !pip install torchaudio\n",
        "    !pip install lightning\n",
        "    !pip install kagglehub\n",
        "    !pip install scikit-learn\n",
        "    !pip install ipython\n",
        "    !pip install soundfile\n",
        "    !pip install wandb\n",
        "    !pip install onnx onnxscript onnxruntime\n",
        "\n",
        "    !pip install \"resampy>=0.4.0\"\n",
        "    !pip install numpy scipy tqdm requests julius\n",
        "    !pip install torchopenl3 --no-deps\n",
        "    !pip install torchcodec\n",
        "else:\n",
        "    print(\"Jestem na lokalnym komputerze. Pomijam ten kod.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc91345",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbc91345",
        "outputId": "507b4cd7-3b0b-439a-b8aa-d8c2b968db05"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import LightningDataModule\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torchmetrics\n",
        "import torch.optim as optim\n",
        "import torchopenl3\n",
        "import numpy as np\n",
        "import wandb\n",
        "from IPython.display import Audio\n",
        "from tqdm.auto import tqdm\n",
        "import gdown\n",
        "import glob\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd2bab5",
      "metadata": {
        "id": "6bd2bab5"
      },
      "source": [
        "Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4f0a777",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4f0a777",
        "outputId": "8d12065f-7bca-4b01-deed-cef47ac1d089"
      },
      "outputs": [],
      "source": [
        "target_dir = \"dataset\"\n",
        "\n",
        "if os.path.exists(target_dir) and len(os.listdir(target_dir)) > 0:\n",
        "    print(f\"Dataset już istnieje w folderze '{target_dir}'. Pomijam pobieranie.\")\n",
        "else:\n",
        "    print(\"Dataset nie znaleziony. Rozpoczynam pobieranie...\")\n",
        "    path = kagglehub.dataset_download(\"junewookim/mad-dataset-military-audio-dataset\")\n",
        "    print(\"Cache KaggleHub:\", path)\n",
        "\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    shutil.copytree(path, target_dir, dirs_exist_ok=True)\n",
        "    print(\"Pobrano i zapisano do:\", target_dir)\n",
        "\n",
        "noise_folder = \"dataset/noises\"\n",
        "os.makedirs(noise_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oIKVVPFV-2Xf",
      "metadata": {
        "id": "oIKVVPFV-2Xf"
      },
      "source": [
        "### Pobieranie szumów z dysku google\n",
        "Ten fragment ma za zadanie pobrać szumy na colaba z dysku google jeżeli jakiegoś dzwięku szumów nie ma w docelowym folderze. Gdy ktoś chce dodać inne szumy to należy dodać je do folderu pod tym adresem URL: https://drive.google.com/drive/folders/14Q_0KNDXACkFQ2oTF1T-gnjIaNbNuaKL?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1-q8o1te8L5t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-q8o1te8L5t",
        "outputId": "6a6c73ac-cd1c-44ff-c016-87a65a2164d8"
      },
      "outputs": [],
      "source": [
        "url = \"https://drive.google.com/drive/folders/14Q_0KNDXACkFQ2oTF1T-gnjIaNbNuaKL?usp=sharing\"\n",
        "output_folder = \"dataset/noises\"\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "existing_wavs = list(Path(output_folder).glob(\"*.wav\"))\n",
        "\n",
        "if len(existing_wavs) > 0:\n",
        "    print(f\"Folder {output_folder} zawiera już {len(existing_wavs)} plików. Pomijam pobieranie.\")\n",
        "else:\n",
        "    print(\"Folder pusty. Rozpoczynam pobieranie szumów z Google Drive...\")\n",
        "    try:\n",
        "        gdown.download_folder(url, output=output_folder, quiet=False, use_cookies=False)\n",
        "        print(\"Pobieranie zakończone sukcesem.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Wystąpił błąd podczas pobierania: {e}\")\n",
        "        print(\"Upewnij się, że link na Google Drive jest ustawiony jako 'Każdy mający link' (Anyone with the link).\")\n",
        "\n",
        "noise_files_list = list(Path(output_folder).glob(\"*.wav\"))\n",
        "print(f\"Gotowe. Dostępnych plików szumu do treningu: {len(noise_files_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3908d177",
      "metadata": {
        "id": "3908d177"
      },
      "source": [
        "### Funkcja dodająca szum\n",
        "Funkcja pomocnicza do augmentacji danych poprzez dodawanie szumu do nagrań."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49651be6",
      "metadata": {
        "id": "49651be6"
      },
      "outputs": [],
      "source": [
        "def add_noise_from_folder(waveform, noise_files, noise_std=0.01):\n",
        "    \"\"\"\n",
        "    waveform: [1, N]\n",
        "    noise_files: lista ścieżek do pliku z szumami\n",
        "    \"\"\"\n",
        "    noise_path = random.choice(noise_files)\n",
        "    noise_waveform, _ = torchaudio.load(noise_path)\n",
        "\n",
        "    L = waveform.shape[1]\n",
        "\n",
        "    # jeśli szum krótszy niż próbka -> powielamy\n",
        "    if noise_waveform.shape[1] < L:\n",
        "        repeats = int(L / noise_waveform.shape[1]) + 1\n",
        "        noise_waveform = noise_waveform.repeat(1, repeats)\n",
        "    # jeśli szum dłuższy -> losowy fragment\n",
        "    if noise_waveform.shape[1] > L:\n",
        "        start = random.randint(0, noise_waveform.shape[1] - L)\n",
        "        noise_waveform = noise_waveform[:, start:start+L]\n",
        "\n",
        "    noise_waveform = noise_waveform / (noise_waveform.std() + 1e-9) * noise_std\n",
        "    noisy = waveform + noise_waveform\n",
        "    return noisy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WemQAI-u2CWV",
      "metadata": {
        "id": "WemQAI-u2CWV"
      },
      "source": [
        "### Klasa ekstraktora cech OpenL3\n",
        "Klasa wrapper dla modelu OpenL3 służąca do ekstrakcji embeddingów z plików audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "316tU6uk16_t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "316tU6uk16_t",
        "outputId": "1142abfb-7e85-471f-f241-2ca78b626f62"
      },
      "outputs": [],
      "source": [
        "class OpenL3FeatureExtractor:\n",
        "    \"\"\"\n",
        "    Ekstraktor cech audio za pomocą modelu torchopenl3 (PyTorch)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_repr=\"mel128\", content_type=\"music\", embedding_size=512):\n",
        "        \"\"\"\n",
        "        input_repr: \"mel128\" lub \"mel256\"\n",
        "        content_type: \"music\" lub \"env\" (w torchopenl3 'environmental' to 'env')\n",
        "        embedding_size: 512 lub 6144\n",
        "        \"\"\"\n",
        "        if content_type == \"environmental\":\n",
        "            content_type = \"env\"\n",
        "\n",
        "        print(f\"Ładowanie modelu TorchOpenL3: {input_repr}, {content_type}, embedding_size={embedding_size}\")\n",
        "\n",
        "        self.model = torchopenl3.models.load_audio_embedding_model(\n",
        "            input_repr=input_repr,\n",
        "            content_type=content_type,\n",
        "            embedding_size=embedding_size\n",
        "        )\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        self.input_repr = input_repr\n",
        "        self.content_type = content_type\n",
        "        self.embedding_size = embedding_size\n",
        "        self.sample_rate = 48000\n",
        "\n",
        "    def extract_features(self, waveform):\n",
        "        \"\"\"\n",
        "        Ekstrakcja cech z audio.\n",
        "        waveform: tensor PyTorch [batch, channels, samples] lub [channels, samples]\n",
        "        \"\"\"\n",
        "\n",
        "        if waveform.dim() == 2:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        waveform = waveform.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embeddings, _ = torchopenl3.get_audio_embedding(\n",
        "                waveform,\n",
        "                sr=48000,\n",
        "                model=self.model,\n",
        "                hop_size=0.1,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "        aggregated = embeddings.mean(dim=1)\n",
        "\n",
        "        return aggregated.cpu()\n",
        "\n",
        "print(\"Inicjalizacja TorchOpenL3 Feature Extractor...\")\n",
        "feature_extractor = OpenL3FeatureExtractor(\n",
        "    input_repr=\"mel128\",\n",
        "    content_type=\"music\",\n",
        "    embedding_size=512\n",
        ")\n",
        "print(f\"OpenL3 ekstraktor załadowany na urządzeniu: {feature_extractor.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rPmPApCn2Omb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "1b578aa49a26416ebdb9fee58d55e59a",
            "90cc8357422b434786d858871e786ccb",
            "7d85769bac6c4d51a684e5a3c64ee542",
            "32a4670f978c4889856ed47b3a358c4e",
            "39187f6222e6486785ef3f06f74ce743",
            "3f1fdc9d6f074d5081546400145959f2",
            "408027feaf63403eadc6874ec82c8fa7",
            "e9cd13027e3a42849a9d52e8867cfc4a",
            "fcc3d612e8cf46bb83c5e6c7861ddc72",
            "1456c2f87c144cc2add97980d076dcb6",
            "6c50b204cb2a494087544f41f7cbd992"
          ]
        },
        "id": "rPmPApCn2Omb",
        "outputId": "2d3eeb3e-8567-415d-d458-69ba288b61ac"
      },
      "outputs": [],
      "source": [
        "features_cache_path = \"dataset/features_cache\"\n",
        "\n",
        "if os.path.exists(features_cache_path):\n",
        "    print(f\"Znaleziono stary folder {features_cache_path} - USUWANIE...\")\n",
        "    shutil.rmtree(features_cache_path) # Usuwa folder i wszystko co w nim jest\n",
        "    print(\"Folder usunięty.\")\n",
        "else:\n",
        "    print(\"Folder cache nie istnieje (to dobrze, zostanie utworzony).\")\n",
        "\n",
        "\n",
        "def precompute_features_dataset(df, root_dir, output_dir, feature_extractor, noise_files):\n",
        "    clean_dir = os.path.join(output_dir, \"clean\")\n",
        "    noisy_dir = os.path.join(output_dir, \"noisy\")\n",
        "    os.makedirs(clean_dir, exist_ok=True)\n",
        "    os.makedirs(noisy_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Generowanie cech do folderu: {output_dir} ...\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        filename = str(row['path'])\n",
        "        src_path = os.path.join(root_dir, filename)\n",
        "\n",
        "        save_name = filename.replace(\"/\", \"_\").replace(\".wav\", \".pt\")\n",
        "        path_clean = os.path.join(clean_dir, save_name)\n",
        "        path_noisy = os.path.join(noisy_dir, save_name)\n",
        "\n",
        "        if os.path.exists(path_clean) and os.path.exists(path_noisy):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # 1. Wczytaj\n",
        "            wav, sr = torchaudio.load(src_path)\n",
        "            # Resample do 48k (OpenL3 wymaga 48k)\n",
        "            if sr != 48000:\n",
        "                wav = torchaudio.transforms.Resample(sr, 48000)(wav)\n",
        "\n",
        "            # 2. Pad/Trim\n",
        "            max_len = 160000\n",
        "            if wav.shape[1] > max_len:\n",
        "                wav = wav[:, :max_len]\n",
        "            else:\n",
        "                wav = F.pad(wav, (0, max_len - wav.shape[1]))\n",
        "\n",
        "            # 3. Zaszumianie\n",
        "            wav_noisy = wav.clone()\n",
        "            if noise_files:\n",
        "                wav_noisy = add_noise_from_folder(wav_noisy, noise_files, noise_std=0.01)\n",
        "\n",
        "            # 4. Ekstrakcja\n",
        "            with torch.no_grad():\n",
        "                # extract_features zwraca już [1, 512] (bo ma w sobie mean)\n",
        "                f_clean = feature_extractor.extract_features(wav.unsqueeze(0))\n",
        "                f_noisy = feature_extractor.extract_features(wav_noisy.unsqueeze(0))\n",
        "\n",
        "                # POPRAWKA: Tylko squeeze, BEZ drugiego mean()\n",
        "                f_clean = f_clean.squeeze(0).cpu() # Teraz [512]\n",
        "                f_noisy = f_noisy.squeeze(0).cpu() # Teraz [512]\n",
        "\n",
        "            # 5. Zapis\n",
        "            torch.save(f_clean, path_clean)\n",
        "            torch.save(f_noisy, path_noisy)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error {filename}: {e}\")\n",
        "\n",
        "df_full = pd.read_csv(\"dataset/MAD_dataset/training.csv\")\n",
        "noise_folder_path = \"dataset/MAD_dataset/noise\"\n",
        "noise_files_list = list(Path(noise_folder_path).glob(\"*.wav\")) if os.path.exists(noise_folder_path) else []\n",
        "\n",
        "# Wywołanie funkcji (ona sama stworzy folder dzięki os.makedirs wewnątrz)\n",
        "precompute_features_dataset(df_full, \"dataset/MAD_dataset\", features_cache_path, feature_extractor, noise_files_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795d1b31",
      "metadata": {
        "id": "795d1b31"
      },
      "source": [
        "### Klasa Dataset na surowym audio\n",
        "Podstawowa klasa Dataset zwracająca pary nagrań (surowe waveformy) oraz informację czy należą do tej samej klasy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2fdc752",
      "metadata": {
        "id": "a2fdc752"
      },
      "outputs": [],
      "source": [
        "class SiameseAudioDataset(Dataset):\n",
        "    def __init__(self, df, features_dir):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.features_dir = features_dir\n",
        "        self.clean_dir = os.path.join(features_dir, \"clean\")\n",
        "        self.noisy_dir = os.path.join(features_dir, \"noisy\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row_a = self.df.iloc[idx]\n",
        "        label_a = int(row_a['label'])\n",
        "        fname_a = str(row_a['path']).replace(\"/\", \"_\").replace(\".wav\", \".pt\")\n",
        "\n",
        "        # A: Zawsze czysta próbka (Anchor)\n",
        "        path_a = os.path.join(self.clean_dir, fname_a)\n",
        "        feat_a = torch.load(path_a) if os.path.exists(path_a) else torch.zeros(512)\n",
        "\n",
        "        # B: Losowanie pary\n",
        "        if random.random() > 0.5:\n",
        "            # Positive (ta sama klasa)\n",
        "            # Bierzemy ZASZUMIONĄ wersję (żeby model uczył się odporności)\n",
        "            target_df = self.df[self.df['label'] == label_a]\n",
        "            same_label = 1\n",
        "        else:\n",
        "            # Negative (inna klasa)\n",
        "            target_df = self.df[self.df['label'] != label_a]\n",
        "            same_label = 0\n",
        "\n",
        "        row_b = target_df.sample(1).iloc[0]\n",
        "        label_b = int(row_b['label'])\n",
        "        fname_b = str(row_b['path']).replace(\"/\", \"_\").replace(\".wav\", \".pt\")\n",
        "\n",
        "        # B: Zaszumiona próbka\n",
        "        path_b = os.path.join(self.noisy_dir, fname_b)\n",
        "        feat_b = torch.load(path_b) if os.path.exists(path_b) else torch.zeros(512)\n",
        "\n",
        "        # Dummy waveform (żeby collate function nie wybuchła, bo spodziewa się czegoś na poz 0 i 2)\n",
        "        dummy = torch.empty(1)\n",
        "\n",
        "        # Zwracamy features na pozycjach 6 i 7 (tak jak było w Twoim kodzie po modyfikacji w collate)\n",
        "        return dummy, label_a, dummy, label_b, same_label, 48000, feat_a, feat_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6d2069",
      "metadata": {
        "id": "9f6d2069"
      },
      "outputs": [],
      "source": [
        "def siamese_collate(batch):\n",
        "    a = torch.stack([item[0] for item in batch])\n",
        "    label_a = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
        "    b = torch.stack([item[2] for item in batch])\n",
        "    label_b = torch.tensor([item[3] for item in batch], dtype=torch.long)\n",
        "    same_label = torch.tensor([item[4] for item in batch], dtype=torch.long)\n",
        "    sample_rate = batch[0][5]\n",
        "\n",
        "    features_a = torch.stack([item[6] for item in batch]) if batch[0][6] is not None else None\n",
        "    features_b = torch.stack([item[7] for item in batch]) if batch[0][7] is not None else None\n",
        "\n",
        "    return a, label_a, b, label_b, same_label, sample_rate, features_a, features_b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c36865f",
      "metadata": {
        "id": "9c36865f"
      },
      "source": [
        "### DataModule dla surowego audio\n",
        "Klasa LightningDataModule zarządzająca datasetami treningowymi i walidacyjnymi dla surowego audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "544f1ea8",
      "metadata": {
        "id": "544f1ea8"
      },
      "outputs": [],
      "source": [
        "class SiameseAudioDataModule(LightningDataModule):\n",
        "    def __init__(self, df, features_dir, batch_size=32, num_workers=4):\n",
        "        super().__init__()\n",
        "        self.df = df\n",
        "        self.features_dir = features_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        train_df, val_df = train_test_split(self.df, test_size=0.2, random_state=42, stratify=self.df['label'])\n",
        "        self.train_ds = SiameseAudioDataset(train_df, self.features_dir)\n",
        "        self.val_ds = SiameseAudioDataset(val_df, self.features_dir)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True,\n",
        "                          num_workers=self.num_workers, collate_fn=siamese_collate)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=False,\n",
        "                          num_workers=self.num_workers, collate_fn=siamese_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184bfa04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "184bfa04",
        "outputId": "593e38b4-15f3-44b1-e92b-408d9482b295"
      },
      "outputs": [],
      "source": [
        "dm_test = SiameseAudioDataModule(df_full, features_cache_path, batch_size=8)\n",
        "dm_test.setup()\n",
        "batch = next(iter(dm_test.train_dataloader()))\n",
        "print(\"Features shape:\", batch[6].shape) # Powinno być [8, 512]\n",
        "print(\"Działa!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df814cfe",
      "metadata": {
        "id": "df814cfe"
      },
      "source": [
        "### Model Syjamski (LightningModule)\n",
        "Definicja modelu sieci neuronowej (klasyfikatora), który przyjmuje różnicę cech dwóch nagrań i decyduje czy są to te same klasy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8662a542",
      "metadata": {
        "id": "8662a542"
      },
      "outputs": [],
      "source": [
        "class SiameseComparator(pl.LightningModule):\n",
        "    def __init__(self, input_dim=512, hidden_dim=256, learning_rate=1e-3):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.loss_fn = nn.BCELoss()\n",
        "\n",
        "        self.accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
        "        self.f1_score = torchmetrics.F1Score(task=\"binary\")\n",
        "\n",
        "    def forward(self, feat_a, feat_b):\n",
        "        diff = torch.abs(feat_a - feat_b)\n",
        "\n",
        "        return self.classifier(diff)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        _, _, _, _, same_label, _, features_a, features_b = batch\n",
        "\n",
        "        probs = self(features_a, features_b)\n",
        "        probs = probs.squeeze()\n",
        "\n",
        "        loss = self.loss_fn(probs, same_label.float())\n",
        "\n",
        "        preds = (probs > 0.5).long()\n",
        "        acc = self.accuracy(preds, same_label)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, _, _, _, same_label, _, features_a, features_b = batch\n",
        "\n",
        "        probs = self(features_a, features_b)\n",
        "        probs = probs.squeeze()\n",
        "\n",
        "        loss = self.loss_fn(probs, same_label.float())\n",
        "\n",
        "        preds = (probs > 0.5).long()\n",
        "        acc = self.accuracy(preds, same_label)\n",
        "        f1 = self.f1_score(preds, same_label)\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "        self.log(\"val_f1\", f1, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"monitor\": \"val_loss\"\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c6444c",
      "metadata": {
        "id": "e8c6444c"
      },
      "source": [
        "### Trening modelu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jb5tbFRS3iv4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "982c4f019d0742a2ba1b22f326421fbe",
            "665429d7db8e46d1a58f5223605e324f"
          ]
        },
        "id": "jb5tbFRS3iv4",
        "outputId": "5dd451af-8475-4bdd-8e84-d0ced4a6f7f8"
      },
      "outputs": [],
      "source": [
        "dm = SiameseAudioDataModule(\n",
        "    df=df_full,  # Używamy df_full zdefiniowanego wcześniej\n",
        "    features_dir=\"dataset/features_cache\", # Folder z wygenerowanymi cechami\n",
        "    batch_size=32,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "# 2. Inicjalizacja modelu\n",
        "model = SiameseComparator(input_dim=512, hidden_dim=256, learning_rate=0.001)\n",
        "\n",
        "# 3. Logger\n",
        "wandb_logger = WandbLogger(\n",
        "    project=\"siamese-audio-classifier\",\n",
        "    name=\"Kamil_Maj_1\",\n",
        "    log_model=\"all\"\n",
        ")\n",
        "\n",
        "# 4. Trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=20,\n",
        "    accelerator=\"auto\",\n",
        "    devices=1,\n",
        "    logger=wandb_logger,\n",
        "    log_every_n_steps=5\n",
        ")\n",
        "\n",
        "print(\"Rozpoczynam trening z logowaniem do W&B...\")\n",
        "# Teraz zmienna 'dm' już istnieje, więc to zadziała:\n",
        "trainer.fit(model, datamodule=dm)\n",
        "\n",
        "print(\"Trening zakończony!\")\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6uWTIQlJDJy4",
      "metadata": {
        "id": "6uWTIQlJDJy4"
      },
      "source": [
        "**Zapisywanie do ONNX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Esa5477ICOji",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Esa5477ICOji",
        "outputId": "483969e3-1f62-4f4a-c59b-057fede284d5"
      },
      "outputs": [],
      "source": [
        "search_pattern = \"siamese-audio-classifier/**/*.ckpt\"\n",
        "list_of_files = glob.glob(search_pattern, recursive=True)\n",
        "\n",
        "if not list_of_files:\n",
        "    print(\"Nie znaleziono checkpointu .ckpt do eksportu!\")\n",
        "else:\n",
        "    latest_checkpoint = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"Eksportuję model z pliku: {latest_checkpoint}\")\n",
        "\n",
        "    device = torch.device(\"cpu\") # Do eksportu ONNX bezpieczniej użyć CPU\n",
        "    model_export = SiameseComparator.load_from_checkpoint(latest_checkpoint)\n",
        "    model_export.to(device)\n",
        "    model_export.eval()\n",
        "\n",
        "    dummy_input_a = torch.randn(1, 512, device=device)\n",
        "    dummy_input_b = torch.randn(1, 512, device=device)\n",
        "\n",
        "    onnx_path = \"siamese_audio_comparator.onnx\"\n",
        "\n",
        "    try:\n",
        "        torch.onnx.export(\n",
        "            model_export,\n",
        "            (dummy_input_a, dummy_input_b),\n",
        "            onnx_path,\n",
        "            export_params=True,\n",
        "            opset_version=12,\n",
        "            do_constant_folding=True,\n",
        "            input_names=['feature_vector_a', 'feature_vector_b'],\n",
        "            output_names=['similarity_score'],\n",
        "            dynamic_axes={\n",
        "                'feature_vector_a': {0: 'batch_size'},\n",
        "                'feature_vector_b': {0: 'batch_size'},\n",
        "                'similarity_score': {0: 'batch_size'}\n",
        "            }\n",
        "        )\n",
        "        print(f\"Sukces! Model zapisany jako: {onnx_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Błąd podczas eksportu do ONNX: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1456c2f87c144cc2add97980d076dcb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b578aa49a26416ebdb9fee58d55e59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90cc8357422b434786d858871e786ccb",
              "IPY_MODEL_7d85769bac6c4d51a684e5a3c64ee542",
              "IPY_MODEL_32a4670f978c4889856ed47b3a358c4e"
            ],
            "layout": "IPY_MODEL_39187f6222e6486785ef3f06f74ce743"
          }
        },
        "32a4670f978c4889856ed47b3a358c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1456c2f87c144cc2add97980d076dcb6",
            "placeholder": "​",
            "style": "IPY_MODEL_6c50b204cb2a494087544f41f7cbd992",
            "value": " 6429/6429 [01:59&lt;00:00, 55.94it/s]"
          }
        },
        "39187f6222e6486785ef3f06f74ce743": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f1fdc9d6f074d5081546400145959f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "408027feaf63403eadc6874ec82c8fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "665429d7db8e46d1a58f5223605e324f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c50b204cb2a494087544f41f7cbd992": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d85769bac6c4d51a684e5a3c64ee542": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9cd13027e3a42849a9d52e8867cfc4a",
            "max": 6429,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcc3d612e8cf46bb83c5e6c7861ddc72",
            "value": 6429
          }
        },
        "90cc8357422b434786d858871e786ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f1fdc9d6f074d5081546400145959f2",
            "placeholder": "​",
            "style": "IPY_MODEL_408027feaf63403eadc6874ec82c8fa7",
            "value": "100%"
          }
        },
        "982c4f019d0742a2ba1b22f326421fbe": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_665429d7db8e46d1a58f5223605e324f",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 19/19 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> 161/161 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">0:00:03 • 0:00:00</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; text-decoration: underline\">55.03it/s</span> <span style=\"font-style: italic\">v_num: xqra train_loss: 0.699     </span>\n                                                                                 <span style=\"font-style: italic\">train_acc: 0.522 val_loss: 0.692  </span>\n                                                                                 <span style=\"font-style: italic\">val_acc: 0.529 val_f1: 0.452      </span>\n</pre>\n",
                  "text/plain": "Epoch 19/19 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 161/161 \u001b[2m0:00:03 • 0:00:00\u001b[0m \u001b[2;4m55.03it/s\u001b[0m \u001b[3mv_num: xqra train_loss: 0.699     \u001b[0m\n                                                                                 \u001b[3mtrain_acc: 0.522 val_loss: 0.692  \u001b[0m\n                                                                                 \u001b[3mval_acc: 0.529 val_f1: 0.452      \u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "e9cd13027e3a42849a9d52e8867cfc4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc3d612e8cf46bb83c5e6c7861ddc72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
