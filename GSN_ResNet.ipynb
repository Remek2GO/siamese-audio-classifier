{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea41522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: lightning in ./.venv/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: kagglehub in ./.venv/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: ipython in ./.venv/lib/python3.10/site-packages (8.37.0)\n",
      "Requirement already satisfied: soundfile in ./.venv/lib/python3.10/site-packages (0.13.1)\n",
      "Requirement already satisfied: wandb in ./.venv/lib/python3.10/site-packages (0.23.1)\n",
      "Requirement already satisfied: gdown in ./.venv/lib/python3.10/site-packages (5.2.0)\n",
      "Requirement already satisfied: torchcodec in ./.venv/lib/python3.10/site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.10/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.10/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: PyYAML<8.0,>5.4 in ./.venv/lib/python3.10/site-packages (from lightning) (6.0.3)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in ./.venv/lib/python3.10/site-packages (from lightning) (0.15.2)\n",
      "Requirement already satisfied: packaging<27.0,>=20.0 in ./.venv/lib/python3.10/site-packages (from lightning) (25.0)\n",
      "Requirement already satisfied: torchmetrics<3.0,>0.7.0 in ./.venv/lib/python3.10/site-packages (from lightning) (1.8.2)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in ./.venv/lib/python3.10/site-packages (from lightning) (4.67.1)\n",
      "Requirement already satisfied: pytorch-lightning in ./.venv/lib/python3.10/site-packages (from lightning) (2.6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.10/site-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.13.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (65.5.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from kagglehub) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.10/site-packages (from ipython) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.10/site-packages (from ipython) (1.3.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.10/site-packages (from ipython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.10/site-packages (from ipython) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.10/site-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.10/site-packages (from ipython) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.10/site-packages (from ipython) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.10/site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in ./.venv/lib/python3.10/site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.14)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.10/site-packages (from soundfile) (2.0.0)\n",
      "Requirement already satisfied: click>=8.0.1 in ./.venv/lib/python3.10/site-packages (from wandb) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./.venv/lib/python3.10/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.10/site-packages (from wandb) (4.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in ./.venv/lib/python3.10/site-packages (from wandb) (6.33.2)\n",
      "Requirement already satisfied: pydantic<3 in ./.venv/lib/python3.10/site-packages (from wandb) (2.12.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./.venv/lib/python3.10/site-packages (from wandb) (2.48.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.10/site-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.10/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->kagglehub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->kagglehub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->kagglehub) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->kagglehub) (2025.11.12)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.10/site-packages (from gdown) (4.14.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.22.0)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.23)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in ./.venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in ./.venv/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.10/site-packages (from stack_data->ipython) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.10/site-packages (from stack_data->ipython) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.10/site-packages (from stack_data->ipython) (0.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instalacje\n",
    "!pip install pandas torch torchaudio lightning kagglehub scikit-learn ipython soundfile wandb gdown torchcodec\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import kagglehub\n",
    "import gdown\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "\n",
    "# Ustawienie ziarna losowoci dla powtarzalnoci\n",
    "pl.seed_everything(42)\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1739a59b",
   "metadata": {},
   "source": [
    "## KOD do uruchamiania na MAD + Generic Audio Dataset\n",
    "\n",
    "przed uruchomieniem tego skryptu nale偶y wykona GAS_integration_script.ipnyb\n",
    "\n",
    "Oba zakadaj 偶e forma dataset贸w w folderze roboczym jest:\n",
    "\n",
    "dataset:\n",
    "\n",
    "        DATASET --- tu jest GAS\n",
    "                Animals\n",
    "                Birds\n",
    "                ...\n",
    "\n",
    "        MAD_dataset \n",
    "\n",
    "        noises\n",
    "\n",
    "        processed_augmented\n",
    "\n",
    "\n",
    "\n",
    "Przed uruchomieniem tego skryptu, podfolder DATASET (z generic audio sounds) powinien by tak: (skrypt integracyjny powinien tak zrobi)\n",
    " \n",
    "DATASET:\n",
    "\n",
    "        Animals\n",
    "        Birds\n",
    "        Environment\n",
    "        Vehicles\n",
    "        gas_test.csv\n",
    "        gas_train.csv\n",
    "        gas.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18425de4",
   "metadata": {},
   "source": [
    "***Integracja dziaania g.collab vs lokalne***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4c539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = os.path.exists('/content')\n",
    "\n",
    "local = not IS_COLAB ## powinno samo wykry\n",
    "if local:\n",
    "    dataset_path = \"dataset\"\n",
    "else:\n",
    "    dataset_path = \"/content/dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8197ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ju偶 istnieje w 'dataset'.\n",
      "Szumy ju偶 istniej w 'dataset/noises'.\n",
      "Liczba dostpnych plik贸w szumu: 5\n"
     ]
    }
   ],
   "source": [
    "# 1. Pobieranie Datasetu MAD\n",
    "target_dir = dataset_path\n",
    "if os.path.exists(target_dir) and len(os.listdir(target_dir)) > 0:\n",
    "    print(f\"Dataset ju偶 istnieje w '{target_dir}'.\")\n",
    "else:\n",
    "    print(\"Pobieranie datasetu MAD...\")\n",
    "    path = kagglehub.dataset_download(\"junewookim/mad-dataset-military-audio-dataset\")\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    shutil.copytree(path, target_dir, dirs_exist_ok=True)\n",
    "    print(\"Pobrano dataset MAD.\")\n",
    "\n",
    "# 2. Pobieranie Szum贸w (z Twojego pliku)\n",
    "noise_folder = dataset_path + \"/noises\"\n",
    "os.makedirs(noise_folder, exist_ok=True)\n",
    "url = \"https://drive.google.com/drive/folders/14Q_0KNDXACkFQ2oTF1T-gnjIaNbNuaKL?usp=sharing\"\n",
    "\n",
    "if not list(Path(noise_folder).glob(\"*.wav\")):\n",
    "    print(\"Pobieranie szum贸w z Google Drive...\")\n",
    "    try:\n",
    "        gdown.download_folder(url, output=noise_folder, quiet=False, use_cookies=False)\n",
    "        print(\"Pobrano szumy.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Bd pobierania szum贸w: {e}\")\n",
    "else:\n",
    "    print(f\"Szumy ju偶 istniej w '{noise_folder}'.\")\n",
    "\n",
    "noise_files = list(glob.glob(os.path.join(noise_folder, \"*.wav\")))\n",
    "print(f\"Liczba dostpnych plik贸w szumu: {len(noise_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d333492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zaadowano DataFrame: 6429 plik贸w.\n",
      "   Unnamed: 0                            path  label  \\\n",
      "0           0  MAD_dataset/training/398/0.wav      3   \n",
      "1           1  MAD_dataset/training/398/1.wav      3   \n",
      "2           2  MAD_dataset/training/398/2.wav      3   \n",
      "3           3  MAD_dataset/training/398/3.wav      3   \n",
      "4           4  MAD_dataset/training/398/4.wav      3   \n",
      "\n",
      "                                       youtube title  \\\n",
      "0  US Army, NATO. Powerful M1A2 Abrams and TR-85 ...   \n",
      "1  US Army, NATO. Powerful M1A2 Abrams and TR-85 ...   \n",
      "2  US Army, NATO. Powerful M1A2 Abrams and TR-85 ...   \n",
      "3  US Army, NATO. Powerful M1A2 Abrams and TR-85 ...   \n",
      "4  US Army, NATO. Powerful M1A2 Abrams and TR-85 ...   \n",
      "\n",
      "                                        youtube url source  \n",
      "0  https://www.youtube.com/watch?v=Oh4q6ck6ufc&t=1s    MAD  \n",
      "1  https://www.youtube.com/watch?v=Oh4q6ck6ufc&t=1s    MAD  \n",
      "2  https://www.youtube.com/watch?v=Oh4q6ck6ufc&t=1s    MAD  \n",
      "3  https://www.youtube.com/watch?v=Oh4q6ck6ufc&t=1s    MAD  \n",
      "4  https://www.youtube.com/watch?v=Oh4q6ck6ufc&t=1s    MAD  \n"
     ]
    }
   ],
   "source": [
    "csv_path = dataset_path + \"/MAD_dataset/training.csv\"\n",
    "df_full = pd.read_csv(csv_path)\n",
    "\n",
    "# Mapowanie nazw kolumn\n",
    "rename_map = {\n",
    "    'filename': 'path',\n",
    "    'class': 'label',\n",
    "    'class_name': 'label'\n",
    "}\n",
    "df_full = df_full.rename(columns=rename_map)\n",
    "\n",
    "# Funkcja naprawiajca cie偶ki\n",
    "def fix_path_MAD(path):\n",
    "    path = str(path)\n",
    "    if not path.startswith(\"MAD_dataset/training/\"):\n",
    "        return os.path.join(\"MAD_dataset\", path)\n",
    "    return path\n",
    "\n",
    "df_full['path'] = df_full['path'].apply(fix_path_MAD)\n",
    "print(f\"Zaadowano DataFrame: {len(df_full)} plik贸w.\")\n",
    "\n",
    "#print head\n",
    "print(df_full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "685ec8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           path  label label_name source\n",
      "0              DATASET/Vehicles/bus/bus_282.wav     24        bus    GAS\n",
      "1     DATASET/Animals/horse/horse_2_part_11.wav     10      horse    GAS\n",
      "2             DATASET/Vehicles/bus/bus_3366.wav     24        bus    GAS\n",
      "3  DATASET/Environment/wind/wind_1_part_460.wav     20       wind    GAS\n",
      "4        DATASET/Birds/crow/crow_4_part_280.wav     12       crow    GAS\n",
      "czna liczba pr贸bek treningowych: 24185\n",
      "czna liczba pr贸bek testowych: 5476\n"
     ]
    }
   ],
   "source": [
    "df_train_gas = pd.read_csv(\"dataset/DATASET/gas_train.csv\")\n",
    "df_test_gas  = pd.read_csv(\"dataset/DATASET/gas_test.csv\")\n",
    "\n",
    "#print train head\n",
    "print(df_train_gas.head())\n",
    "\n",
    "\n",
    "df_full_train = pd.concat([df_full, df_train_gas], ignore_index=True)\n",
    "df_test_mad = pd.read_csv(\"dataset/MAD_dataset/test.csv\")\n",
    "\n",
    "#fix test mad paths\n",
    "\n",
    "df_test_mad['path'] = df_test_mad['path'].apply(fix_path_MAD)\n",
    "\n",
    "df_full_test = pd.concat([df_test_mad, df_test_gas], ignore_index=True)\n",
    "print(f\"czna liczba pr贸bek treningowych: {len(df_full_train)}\")\n",
    "print(f\"czna liczba pr贸bek testowych: {len(df_full_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cbbffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                            path  label  \\\n",
      "0         0.0  MAD_dataset/training/398/0.wav      3   \n",
      "1         1.0  MAD_dataset/training/398/1.wav      3   \n",
      "2         2.0  MAD_dataset/training/398/2.wav      3   \n",
      "3         3.0  MAD_dataset/training/398/3.wav      3   \n",
      "4         4.0  MAD_dataset/training/398/4.wav      3   \n",
      "\n",
      "                                       youtube title  \\\n",
      "0  US Army, NATO. Powerful M1A2 Abrams and TR-85 ...   \n",
      "1  US Army, NATO. Powerful M1A2 Abrams and TR-85 ...   \n",
      "2  US Army, NATO. Powerful M1A2 Abrams and TR-85 ...   \n",
      "3  US Army, NATO. Powerful M1A2 Abrams and TR-85 ...   \n",
      "4  US Army, NATO. Powerful M1A2 Abrams and TR-85 ...   \n",
      "\n",
      "                                        youtube url source label_name  \n",
      "0  https://www.youtube.com/watch?v=Oh4q6ck6ufc&t=1s    MAD        NaN  \n",
      "1  https://www.youtube.com/watch?v=Oh4q6ck6ufc&t=1s    MAD        NaN  \n",
      "2  https://www.youtube.com/watch?v=Oh4q6ck6ufc&t=1s    MAD        NaN  \n",
      "3  https://www.youtube.com/watch?v=Oh4q6ck6ufc&t=1s    MAD        NaN  \n",
      "4  https://www.youtube.com/watch?v=Oh4q6ck6ufc&t=1s    MAD        NaN  \n"
     ]
    }
   ],
   "source": [
    "#print new head\n",
    "print(df_full_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fdafda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CachedAudioDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, noise_files=None, training=True, target_len=150000, expansion_factor=1):\n",
    "        \"\"\"\n",
    "        expansion_factor: Ile razy powieli dataset w jednej epoce.\n",
    "        Np. expansion_factor=5 sprawi, 偶e dataset 6000 plik贸w bdzie \"widziany\" jako 30000.\n",
    "        Ka偶da kopia dostanie inn, losow augmentacj.\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root_dir = os.path.abspath(str(root_dir).strip())\n",
    "        self.noise_files = noise_files\n",
    "        self.training = training\n",
    "        self.target_len = target_len\n",
    "        self.expansion_factor = expansion_factor if training else 1\n",
    "        self.target_sr = 48000\n",
    "\n",
    "        self.labels_to_indices = self.df.groupby('label').groups\n",
    "        self.all_labels = list(self.labels_to_indices.keys())\n",
    "\n",
    "        self.sources = self.df['source'].values\n",
    "        self.source_to_indices = self.df.groupby('source').groups\n",
    "\n",
    "        self.label_counts = {\n",
    "            label: len(indices)\n",
    "            for label, indices in self.labels_to_indices.items()\n",
    "        }\n",
    "\n",
    "        self.neg_label_weights = {\n",
    "            label: 1.0 / count\n",
    "            for label, count in self.label_counts.items()\n",
    "        }\n",
    "\n",
    "        self._neg_labels, self._neg_weights = zip(*self.neg_label_weights.items())\n",
    "        \n",
    "\n",
    "        # 1. Cache SZUMW\n",
    "        self.cached_noises = []\n",
    "        if noise_files:\n",
    "            print(f\"Cache'owanie {len(noise_files)} plik贸w szumu...\")\n",
    "            for nf in noise_files:\n",
    "                try:\n",
    "                    wav, sr = torchaudio.load(nf)\n",
    "                    if sr != self.target_sr: wav = T.Resample(sr, self.target_sr)(wav)\n",
    "                    if wav.shape[0] > 1: wav = wav.mean(dim=0, keepdim=True)\n",
    "                    self.cached_noises.append(wav)\n",
    "                except: pass\n",
    "\n",
    "        # 2. Cache DANYCH TRENINGOWYCH\n",
    "        print(f\"adowanie {len(self.df)} plik贸w treningowych z: {self.root_dir}\")\n",
    "        self.audio_cache = []\n",
    "        errors = 0\n",
    "        for i, row in enumerate(self.df.itertuples()):\n",
    "            csv_path = str(row.path).strip()\n",
    "            #full_path = os.path.join(self.root_dir, csv_path)\n",
    "            full_path = self.resolve_path(csv_path)\n",
    "\n",
    "\n",
    "\n",
    "            try:\n",
    "                if not os.path.exists(full_path):\n",
    "                    raise FileNotFoundError(\"Plik nie istnieje\")\n",
    "\n",
    "                wav, sr = torchaudio.load(full_path)\n",
    "                if sr != self.target_sr: wav = T.Resample(sr, self.target_sr)(wav)\n",
    "                if wav.shape[0] > 1: wav = wav.mean(dim=0, keepdim=True)\n",
    "\n",
    "                if wav.shape[1] < self.target_len:\n",
    "                    wav = F.pad(wav, (0, self.target_len - wav.shape[1]))\n",
    "                elif wav.shape[1] > self.target_len:\n",
    "                    start = (wav.shape[1] - self.target_len) // 2\n",
    "                    wav = wav[:, start:start+self.target_len]\n",
    "\n",
    "                self.audio_cache.append(wav)\n",
    "\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                self.audio_cache.append(torch.randn(1, self.target_len) * 0.001)\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"  bd adowania pliku {i}/{len(self.df)}: {full_path} ({e})\")\n",
    "\n",
    "        if errors > 0:\n",
    "            print(f\" uwaga: {errors} plik贸w nie zaadowano (wstawiono szum).\")\n",
    "\n",
    "        else:\n",
    "            print(\"sukces: Wszystkie pliki w pamici RAM.\")\n",
    "\n",
    "        if self.training and self.expansion_factor > 1:\n",
    "            print(f\" DATASET ROZSZERZONY: {len(self.df)} plik贸w -> {len(self)} wirtualnych pr贸bek na epok.\")\n",
    "\n",
    "    def aggressive_augment(self, waveform):\n",
    "        # Prosta agresywna augmentacja\n",
    "        gain = random.uniform(0.5, 1.5)\n",
    "        waveform = waveform * gain\n",
    "\n",
    "\n",
    "        #extra masking\n",
    "        #freq_mask = T.FrequencyMasking(freq_mask_param=5)\n",
    "        #time_mask = T.TimeMasking(time_mask_param=10)\n",
    "        #waveform = freq_mask(waveform)\n",
    "        #waveform = time_mask(waveform)\n",
    "\n",
    "        if self.cached_noises and random.random() > 0.3:\n",
    "            noise_wav = random.choice(self.cached_noises)\n",
    "            sig_len = waveform.shape[1]\n",
    "            if noise_wav.shape[1] < sig_len:\n",
    "                repeats = int(sig_len / noise_wav.shape[1]) + 1\n",
    "                curr_noise = noise_wav.repeat(1, repeats)[:, :sig_len]\n",
    "            else:\n",
    "                start = random.randint(0, noise_wav.shape[1] - sig_len)\n",
    "                curr_noise = noise_wav[:, start:start+sig_len]\n",
    "            snr_db = random.uniform(5.0, 25.0)\n",
    "            signal_power = waveform.norm(p=2)\n",
    "            noise_power = curr_noise.norm(p=2)\n",
    "            if noise_power > 0:\n",
    "                snr = 10 ** (snr_db / 20)\n",
    "                scale = signal_power / (noise_power * snr + 1e-9)\n",
    "                waveform = waveform + (curr_noise * scale)\n",
    "        return waveform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Dataset udaje, 偶e jest wikszy ni偶 w rzeczywistoci\n",
    "        return len(self.df) * self.expansion_factor\n",
    "    \n",
    "    def resolve_path(self, csv_path):\n",
    "        return os.path.join(self.root_dir, csv_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Mapujemy wirtualny indeks na prawdziwy\n",
    "        real_idx = idx % len(self.df)\n",
    "\n",
    "        wav_a = self.audio_cache[real_idx].clone()\n",
    "        label_a = self.df.iloc[real_idx]['label']\n",
    "\n",
    "        if self.training:\n",
    "            wav_a = self.aggressive_augment(wav_a)\n",
    "\n",
    "        # Positive\n",
    "        # idxs_p = self.labels_to_indices[label_a]\n",
    "        # possible_p = idxs_p.drop(real_idx, errors='ignore')\n",
    "        # idx_p = random.choice(possible_p) if len(possible_p) > 0 else real_idx\n",
    "\n",
    "        idxs_p = self.labels_to_indices[label_a]\n",
    "\n",
    "        if label_a == 5:  # helicopter\n",
    "            # allow cross-dataset positives\n",
    "            possible_p = idxs_p.drop(real_idx, errors='ignore')\n",
    "        else:\n",
    "            # same-dataset positives only\n",
    "            source_a = self.sources[real_idx]\n",
    "            possible_p = [\n",
    "                i for i in idxs_p\n",
    "                if i != real_idx and self.sources[i] == source_a\n",
    "            ]\n",
    "\n",
    "        idx_p = random.choice(possible_p) if len(possible_p) > 0 else real_idx\n",
    "\n",
    "\n",
    "        wav_p = self.audio_cache[idx_p].clone()\n",
    "        if self.training: wav_p = self.aggressive_augment(wav_p)\n",
    "\n",
    "        # Negative\n",
    "        neg_labels = [l for l in self._neg_labels if l != label_a]\n",
    "        neg_weights = [\n",
    "            self.neg_label_weights[l] for l in neg_labels\n",
    "        ]\n",
    "\n",
    "        label_n = random.choices(neg_labels, weights=neg_weights, k=1)[0]\n",
    "\n",
    "\n",
    "        #label_n = random.choice([l for l in self.all_labels if l != label_a])\n",
    "        idx_n = random.choice(self.labels_to_indices[label_n])\n",
    "        wav_n = self.audio_cache[idx_n].clone()\n",
    "        if self.training: wav_n = self.aggressive_augment(wav_n)\n",
    "\n",
    "        return wav_a, wav_p, wav_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f36d18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetTripletGPU(pl.LightningModule):\n",
    "    def __init__(self, df, root_dir, noise_files, margin=0.8, learning_rate=1e-5):\n",
    "        #changed margin from 1.0 to 0.3 (v1 --> v2)\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['df', 'root_dir', 'noise_files'])\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.noise_files = noise_files\n",
    "        self.val_preds = []\n",
    "        self.val_targets = []\n",
    "        self.test_preds = []\n",
    "        self.test_targets = []\n",
    "        self.val_d_ap = []\n",
    "        self.val_d_an = []\n",
    "\n",
    "        #checking early backbone freezing\n",
    "        self.freeze_epochs = 1     # warm-up\n",
    "        self.partial_epochs = 1    # unfreeze layer4 after this\n",
    "\n",
    "\n",
    "        # 1. Transformacja na GPU\n",
    "        self.spec_layer = T.MelSpectrogram(\n",
    "            sample_rate=48000, n_fft=1024, hop_length=512, n_mels=64, f_min=20, f_max=24000\n",
    "        )\n",
    "        self.db_layer = T.AmplitudeToDB()\n",
    "\n",
    "        # 2. Backbone\n",
    "        self.backbone = resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # Dostosowanie ResNet do 1 kanau\n",
    "        original_conv1 = self.backbone.conv1\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.backbone.conv1.weight.data = original_conv1.weight.data.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # V2 version with more complex head\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(256, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(256, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "        # V3 version even bigger head\n",
    "        # self.backbone.fc = nn.Sequential(\n",
    "        #     nn.Linear(512, 1024, bias=False),\n",
    "        #     nn.BatchNorm1d(1024),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Dropout(0.2),\n",
    "        #     nn.Linear(1024, 512, bias=False),\n",
    "        #     nn.BatchNorm1d(512),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Dropout(0.2),\n",
    "        #     nn.Linear(512, 256, bias=False),\n",
    "        #     nn.BatchNorm1d(256),\n",
    "        #     nn.LeakyReLU(0.1, inplace=True),\n",
    "        #     nn.Dropout(0.2),\n",
    "        #     nn.Linear(256, 256, bias=False),\n",
    "        #     nn.BatchNorm1d(256),\n",
    "        #     nn.LeakyReLU(0.1, inplace=True),\n",
    "        #     nn.Linear(256, 128)\n",
    "        # )\n",
    "\n",
    "        # self.backbone.fc = nn.Sequential(\n",
    "        #     nn.Linear(512, 512, bias=False),\n",
    "        #     nn.BatchNorm1d(512),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(512, 256, bias=False),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Dropout(0.2),\n",
    "        #     nn.Linear(256, 256, bias=False),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Linear(256, 128)\n",
    "        # )\n",
    "\n",
    "\n",
    "        self.loss_fn = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "\n",
    "        #soft margin with cosine similarity\n",
    "        #self.loss_fn = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1 - F.cosine_similarity(x, y))\n",
    "\n",
    "    def compute_features(self, wav):\n",
    "        # To dzieje si na GPU!\n",
    "        spec = self.spec_layer(wav)\n",
    "        spec = self.db_layer(spec)\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
    "        return self.backbone(spec)\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        if self.current_epoch == self.freeze_epochs:\n",
    "            # unfreeze layer4 only\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                if \"layer4\" in name or \"fc\" in name:\n",
    "                    param.requires_grad = True\n",
    "            print(\" Unfroze layer4\")\n",
    "\n",
    "        if self.current_epoch == self.partial_epochs:\n",
    "            self.unfreeze_backbone()\n",
    "            print(\" Unfroze full backbone\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(self.compute_features(x), p=2, dim=1)\n",
    "    \n",
    "    def on_fit_start(self):\n",
    "        self.freeze_backbone()\n",
    "        print(\"锔 Zamro偶ono backbone na pocztku treningu\")\n",
    "\n",
    "    #// old version, fallback\n",
    "    # def training_step(self, batch, batch_idx):\n",
    "    #     wav_a, wav_p, wav_n = batch\n",
    "    #     emb_a = self(wav_a)\n",
    "    #     emb_p = self(wav_p)\n",
    "    #     emb_n = self(wav_n)\n",
    "\n",
    "    #     loss = self.loss_fn(emb_a, emb_p, emb_n)\n",
    "    #     acc = (F.pairwise_distance(emb_a, emb_p) < F.pairwise_distance(emb_a, emb_n)).float().mean()\n",
    "    #     self.log(\"train_loss\", loss, prog_bar=True)\n",
    "    #     self.log(\"train_acc\", acc, prog_bar=True)\n",
    "    #     return loss\n",
    "    #//\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        wav_a, wav_p, wav_n = batch\n",
    "\n",
    "        emb_a = self(wav_a)\n",
    "        emb_p = self(wav_p)\n",
    "        emb_n = self(wav_n)\n",
    "\n",
    "        # distances\n",
    "        d_ap = F.pairwise_distance(emb_a, emb_p)          # (B,)\n",
    "        d_an = F.pairwise_distance(emb_a, emb_n)          # (B,)\n",
    "\n",
    "        # SEMI-HARD NEGATIVE MASK\n",
    "        # d_ap < d_an < d_ap + margin\n",
    "        semi_hard_mask = (d_an > d_ap) & (d_an < d_ap + self.loss_fn.margin)\n",
    "\n",
    "        # fallback to hardest negatives if none are semi-hard\n",
    "        if semi_hard_mask.any():\n",
    "            emb_n = emb_n[semi_hard_mask]\n",
    "            emb_a = emb_a[semi_hard_mask]\n",
    "            emb_p = emb_p[semi_hard_mask]\n",
    "        else:\n",
    "            # batch-hard fallback\n",
    "            hard_idx = d_an.argmax()\n",
    "            emb_n = emb_n[hard_idx].unsqueeze(0)\n",
    "            emb_a = emb_a[hard_idx].unsqueeze(0)\n",
    "            emb_p = emb_p[hard_idx].unsqueeze(0)\n",
    "\n",
    "        loss = self.loss_fn(emb_a, emb_p, emb_n)\n",
    "\n",
    "        acc = (F.pairwise_distance(emb_a, emb_p) <\n",
    "            F.pairwise_distance(emb_a, emb_n)).float().mean()\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        wav_a, wav_p, wav_n = batch\n",
    "        emb_a = self(wav_a)\n",
    "        emb_p = self(wav_p)\n",
    "        emb_n = self(wav_n)\n",
    "\n",
    "        d_ap = F.pairwise_distance(emb_a, emb_p)\n",
    "        d_an = F.pairwise_distance(emb_a, emb_n)\n",
    "\n",
    "        self.val_d_ap.append(d_ap.detach())\n",
    "        self.val_d_an.append(d_an.detach())\n",
    "\n",
    "        loss = self.loss_fn(emb_a, emb_p, emb_n)\n",
    "\n",
    "        preds = (d_ap < d_an).long()          # 1 = correct\n",
    "        targets = torch.ones_like(preds)      # always positive\n",
    "\n",
    "        self.val_preds.append(preds)\n",
    "        self.val_targets.append(targets)\n",
    "\n",
    "        acc = preds.float().mean()\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        preds = torch.cat(self.val_preds)\n",
    "        targets = torch.cat(self.val_targets)\n",
    "\n",
    "        tp = ((preds == 1) & (targets == 1)).sum().float()\n",
    "        fp = ((preds == 1) & (targets == 0)).sum().float()\n",
    "        fn = ((preds == 0) & (targets == 1)).sum().float()\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-9)\n",
    "        recall = tp / (tp + fn + 1e-9)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "\n",
    "        self.log(\"val_f1\", f1, prog_bar=True)\n",
    "\n",
    "        self.val_preds.clear()\n",
    "        self.val_targets.clear()\n",
    "\n",
    "        optimizer = self.optimizers()\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        self.log(\"lr\", lr, prog_bar=True)\n",
    "\n",
    "\n",
    "        d_ap = torch.cat(self.val_d_ap)\n",
    "        d_an = torch.cat(self.val_d_an)\n",
    "\n",
    "        gap = d_an.mean() - d_ap.mean()\n",
    "        violation = (d_an < d_ap + self.loss_fn.margin).float().mean()\n",
    "\n",
    "        self.log(\"val_d_ap\", d_ap.mean(), prog_bar=True)\n",
    "        self.log(\"val_d_an\", d_an.mean(), prog_bar=True)\n",
    "        self.log(\"val_gap\", gap, prog_bar=True)\n",
    "        self.log(\"val_margin_violation\", violation, prog_bar=True)\n",
    "\n",
    "        self.val_d_ap.clear()\n",
    "        self.val_d_an.clear()\n",
    "\n",
    "\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     wav_a, wav_p, wav_n = batch\n",
    "    #     emb_a = self(wav_a)\n",
    "    #     emb_p = self(wav_p)\n",
    "    #     emb_n = self(wav_n)\n",
    "\n",
    "    #     d_ap = F.pairwise_distance(emb_a, emb_p)\n",
    "    #     d_an = F.pairwise_distance(emb_a, emb_n)\n",
    "\n",
    "    #     loss = self.loss_fn(emb_a, emb_p, emb_n)\n",
    "\n",
    "    #     preds = (d_ap < d_an).long()\n",
    "    #     targets = torch.ones_like(preds)\n",
    "\n",
    "    #     self.test_preds.append(preds)\n",
    "    #     self.test_targets.append(targets)\n",
    "\n",
    "    #     acc = preds.float().mean()\n",
    "\n",
    "    #     self.log(\"test_loss\", loss)\n",
    "    #     self.log(\"test_acc\", acc)\n",
    "\n",
    "    # def on_test_epoch_end(self):\n",
    "    #     preds = torch.cat(self.test_preds)\n",
    "    #     targets = torch.cat(self.test_targets)\n",
    "\n",
    "    #     tp = ((preds == 1) & (targets == 1)).sum().float()\n",
    "    #     fp = ((preds == 1) & (targets == 0)).sum().float()\n",
    "    #     fn = ((preds == 0) & (targets == 1)).sum().float()\n",
    "\n",
    "    #     precision = tp / (tp + fp + 1e-9)\n",
    "    #     recall = tp / (tp + fn + 1e-9)\n",
    "    #     f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "\n",
    "    #     self.log(\"test_f1\", f1)\n",
    "\n",
    "    #     self.test_preds.clear()\n",
    "    #     self.test_targets.clear()\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        wav, label = batch\n",
    "        emb = self(wav)\n",
    "\n",
    "        return {\"emb\": emb.detach(), \"label\": label}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=self.hparams.learning_rate\n",
    "        )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=0.5,\n",
    "            patience=2\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_df, _ = train_test_split(self.df, test_size=0.2, random_state=42, stratify=self.df['label'])\n",
    "        ds = CachedAudioDataset(\n",
    "            train_df,\n",
    "            self.root_dir,\n",
    "            self.noise_files,\n",
    "            training=True,\n",
    "            expansion_factor=4\n",
    "        )\n",
    "        return DataLoader(ds, batch_size=64, shuffle=True, num_workers=2, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        _, val_df = train_test_split(self.df, test_size=0.2, random_state=42, stratify=self.df['label'])\n",
    "        ds = CachedAudioDataset(val_df, self.root_dir, noise_files=None, training=False)\n",
    "        return DataLoader(ds, batch_size=64, shuffle=False, num_workers=2, persistent_workers=True)\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.backbone.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    \n",
    "\n",
    "    def recall_at_k(embeddings, labels, k=1):\n",
    "        embeddings = F.normalize(embeddings, dim=1)\n",
    "        dists = torch.cdist(embeddings, embeddings)\n",
    "\n",
    "        # exclude self-match\n",
    "        dists.fill_diagonal_(float('inf'))\n",
    "\n",
    "        nn_idx = dists.topk(k, largest=False).indices\n",
    "        nn_labels = labels[nn_idx]\n",
    "\n",
    "        correct = (nn_labels == labels.unsqueeze(1)).any(dim=1)\n",
    "        return correct.float().mean().item()\n",
    "\n",
    "    def mean_average_precision(embeddings, labels):\n",
    "        embeddings = F.normalize(embeddings, dim=1)\n",
    "        dists = torch.cdist(embeddings, embeddings)\n",
    "\n",
    "        N = len(labels)\n",
    "        ap = []\n",
    "\n",
    "        for i in range(N):\n",
    "            dist = dists[i]\n",
    "            order = torch.argsort(dist)\n",
    "            order = order[order != i]\n",
    "\n",
    "            correct = (labels[order] == labels[i]).float()\n",
    "            if correct.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            precision = torch.cumsum(correct, 0) / (torch.arange(len(correct)) + 1)\n",
    "            ap.append((precision * correct).sum() / correct.sum())\n",
    "\n",
    "        return torch.stack(ap).mean().item()\n",
    "\n",
    "\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        emb = torch.cat([x[\"emb\"] for x in self.trainer.callback_metrics])\n",
    "        labels = torch.cat([x[\"label\"] for x in self.trainer.callback_metrics])\n",
    "\n",
    "        r1 = self.recall_at_k(emb, labels, 1)\n",
    "        r5 = self.recall_at_k(emb, labels, 5)\n",
    "        map_score = self.mean_average_precision(emb, labels)\n",
    "\n",
    "        self.log(\"test_recall@1\", r1)\n",
    "        self.log(\"test_recall@5\", r5)\n",
    "        self.log(\"test_mAP\", map_score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4714d369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Checkpointy z tego treningu trafi do: checkpoints/IR_ResNet_Triplet_combined_datasets_v2_2026-01-19_20-31-08\n",
      " Rozpoczynam trening: IR_ResNet_Triplet_combined_datasets_v2_2026-01-19_20-31-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4050 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous setting has no effect and will be removed in a future version.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb/run-20260119_203108-1mxszlkf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deep-neural-network-course/siamese-audio-classifier/runs/1mxszlkf' target=\"_blank\">IR_ResNet_Triplet_combined_datasets_v2_2026-01-19_20-31-08</a></strong> to <a href='https://wandb.ai/deep-neural-network-course/siamese-audio-classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deep-neural-network-course/siamese-audio-classifier' target=\"_blank\">https://wandb.ai/deep-neural-network-course/siamese-audio-classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deep-neural-network-course/siamese-audio-classifier/runs/1mxszlkf' target=\"_blank\">https://wandb.ai/deep-neural-network-course/siamese-audio-classifier/runs/1mxszlkf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params | Mode  | FLOPs\n",
      "-----------------------------------------------------------------\n",
      "0 | spec_layer | MelSpectrogram    | 0      | train | 0    \n",
      "1 | db_layer   | AmplitudeToDB     | 0      | train | 0    \n",
      "2 | backbone   | ResNet            | 11.5 M | train | 0    \n",
      "3 | loss_fn    | TripletMarginLoss | 0      | train | 0    \n",
      "-----------------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "45.867    Total estimated model params size (MB)\n",
      "86        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "锔 Zamro偶ono backbone na pocztku treningu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ab43a7b4724e4ba79b4fd30cbb145c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adowanie 4837 plik贸w treningowych z: /home/iwo/GSN/siamese-audio-classifier/dataset\n",
      "sukces: Wszystkie pliki w pamici RAM.\n",
      "Cache'owanie 5 plik贸w szumu...\n",
      "adowanie 19348 plik贸w treningowych z: /home/iwo/GSN/siamese-audio-classifier/dataset\n",
      "sukces: Wszystkie pliki w pamici RAM.\n",
      " DATASET ROZSZERZONY: 19348 plik贸w -> 77392 wirtualnych pr贸bek na epok.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c18605ba4a44e0989dcfaf56c586a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c4a3988f5b48de921c70e2000b9791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unfroze layer4\n",
      " Unfroze full backbone\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ec2a7f6fd04c3e942d7fe0cdddd103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be4bed201a549e5849318b6bf7a3ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfbad066588466ca2202f3704d3e1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0428dc8e00144a6294f3f31a89f6c16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fc4fc85db341bd94cc4c12cf02b900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05fbafeebe240719766c69ef608224a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8bc19c0283402b99e532e6401ac5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1e6685e12947f3b3130346b9751fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trening zakoczony.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>lr</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_d_an</td><td></td></tr><tr><td>val_d_ap</td><td></td></tr><tr><td>val_f1</td><td></td></tr><tr><td>val_gap</td><td></td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>8</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train_acc</td><td>1</td></tr><tr><td>train_loss</td><td>0</td></tr><tr><td>trainer/global_step</td><td>10889</td></tr><tr><td>val_acc</td><td>0.93922</td></tr><tr><td>val_d_an</td><td>0.96882</td></tr><tr><td>val_d_ap</td><td>0.27628</td></tr><tr><td>val_f1</td><td>0.96866</td></tr><tr><td>val_gap</td><td>0.69253</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">IR_ResNet_Triplet_combined_datasets_v2_2026-01-19_20-31-08</strong> at: <a href='https://wandb.ai/deep-neural-network-course/siamese-audio-classifier/runs/1mxszlkf' target=\"_blank\">https://wandb.ai/deep-neural-network-course/siamese-audio-classifier/runs/1mxszlkf</a><br> View project at: <a href='https://wandb.ai/deep-neural-network-course/siamese-audio-classifier' target=\"_blank\">https://wandb.ai/deep-neural-network-course/siamese-audio-classifier</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>wandb/run-20260119_203108-1mxszlkf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# ==========================================\n",
    "# KONFIGURACJA UNIKALNEGO TRENINGU\n",
    "# ==========================================\n",
    "\n",
    "#OPCJALNIE: customowy dodatek nazy runu\n",
    "model_big_name = \"IR_ResNet_Triplet_combined_datasets_v2\"\n",
    "\n",
    "\n",
    "# 1. Tworzymy unikaln nazw folderu na podstawie daty i godziny\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_name = f\"{model_big_name}_{timestamp}\"\n",
    "checkpoint_dir = os.path.join(\"checkpoints\", run_name)\n",
    "\n",
    "# Tworzymy folder fizycznie\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "print(f\" Checkpointy z tego treningu trafi do: {checkpoint_dir}\")\n",
    "\n",
    "# ==========================================\n",
    "# CALLBACKI I LOGGER\n",
    "# ==========================================\n",
    "\n",
    "# 2. Checkpoint najlepszego modelu wg Accuracy\n",
    "checkpoint_best = ModelCheckpoint(\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    dirpath=checkpoint_dir,\n",
    "    filename=\"best-epoch={epoch:02d}-acc={val_acc:.4f}\",\n",
    "    save_top_k=1,\n",
    "    auto_insert_metric_name=False\n",
    ")\n",
    "\n",
    "# 3. Checkpoint okresowy (co 5 epok)\n",
    "checkpoint_periodic = ModelCheckpoint(\n",
    "    dirpath=checkpoint_dir,\n",
    "    filename=\"periodic-epoch={epoch:02d}\",\n",
    "    every_n_epochs=5,\n",
    "    save_last=True,\n",
    "    save_top_k=-1\n",
    ")\n",
    "\n",
    "#early stopping opcjonalnie\n",
    "early_stopping = pl.callbacks.EarlyStopping(\n",
    "    monitor=\"val_acc\",\n",
    "    patience=5,\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "# 4. WandB Logger\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"siamese-audio-classifier\",\n",
    "    entity=\"deep-neural-network-course\",\n",
    "    name=run_name,\n",
    "    log_model=False\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# START TRENINGU\n",
    "# ==========================================\n",
    "\n",
    "if local:\n",
    "    ROOT_DIR = \"dataset/\"  # upewnij si, 偶e to poprawna cie偶ka!\n",
    "else:\n",
    "    ROOT_DIR = \"/content/dataset/\" # upewnij si, 偶e to poprawna cie偶ka!\n",
    "\n",
    "\n",
    "model = ResNetTripletGPU(\n",
    "    df=df_full_train,\n",
    "    root_dir=ROOT_DIR,\n",
    "    noise_files=noise_files,\n",
    "    margin=0.5\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_best, checkpoint_periodic, early_stopping],\n",
    "    log_every_n_steps=10,\n",
    "    precision=32,\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "print(f\" Rozpoczynam trening: {run_name}\")\n",
    "trainer.fit(model)\n",
    "\n",
    "print(\"Trening zakoczony.\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59ab8ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path to best model: /home/iwo/GSN/siamese-audio-classifier/checkpoints/IR_ResNet_Triplet_combined_datasets_v2_2026-01-19_20-31-08/best-epoch=03-acc=0.9450.ckpt\n",
      "\n",
      " ROZPOCZYNAM TESTOWANIE NAJLEPSZEGO MODELU (Recall@K)...\n",
      " Caching 1037 evaluation files from: /home/iwo/GSN/siamese-audio-classifier/dataset/MAD_dataset\n",
      " All evaluation files cached in RAM\n",
      " Ekstrakcja embedding贸w...\n",
      "憋 Czas testowania: 1640.17 ms\n",
      "憋 redni czas inferencji na pr贸bk: 1.5817 ms\n",
      "\n",
      " WYNIKI BEST:\n",
      "Recall@1  = 0.5931\n",
      "Recall@2  = 0.7223\n",
      "Recall@5  = 0.8505\n",
      "Recall@10 = 0.9180\n",
      "KNN F1 (k=5) = 0.5505\n",
      " Zapisano NAJLEPSZY model: two_datasets_final_v1_R1=0.5931_R5=0.8505_R10=0.9180_time=1.5817.ckpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "def build_embedding_bank(model, dataloader, device):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    for wav, label in dataloader:  # NOTE: test dataset must return (wav, label)\n",
    "        wav = wav.to(device)\n",
    "        emb = model(wav)\n",
    "        embeddings.append(emb.cpu())\n",
    "        labels.append(label)\n",
    "\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    labels = torch.cat(labels)\n",
    "    return embeddings, labels\n",
    "\n",
    "\n",
    "class CachedEvalAudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    EVALUATION / TESTING ONLY\n",
    "    Returns (wav, label)\n",
    "    Compatible with Recall@K, mAP, NN search\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, root_dir, target_len=150000):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root_dir = os.path.abspath(str(root_dir).strip())\n",
    "        self.target_len = target_len\n",
    "        self.target_sr = 48000\n",
    "\n",
    "        print(f\" Caching {len(self.df)} evaluation files from: {self.root_dir}\")\n",
    "\n",
    "        self.audio_cache = []\n",
    "        self.labels = []\n",
    "\n",
    "        errors = 0\n",
    "        for row in self.df.itertuples():\n",
    "            csv_path = str(row.path).strip()\n",
    "            full_path = os.path.join(self.root_dir, csv_path)\n",
    "\n",
    "            try:\n",
    "                wav, sr = torchaudio.load(full_path)\n",
    "\n",
    "                if sr != self.target_sr:\n",
    "                    wav = T.Resample(sr, self.target_sr)(wav)\n",
    "\n",
    "                if wav.shape[0] > 1:\n",
    "                    wav = wav.mean(dim=0, keepdim=True)\n",
    "\n",
    "                if wav.shape[1] < self.target_len:\n",
    "                    wav = F.pad(wav, (0, self.target_len - wav.shape[1]))\n",
    "                elif wav.shape[1] > self.target_len:\n",
    "                    start = (wav.shape[1] - self.target_len) // 2\n",
    "                    wav = wav[:, start:start + self.target_len]\n",
    "\n",
    "            except Exception:\n",
    "                errors += 1\n",
    "                wav = torch.randn(1, self.target_len) * 0.001\n",
    "\n",
    "            self.audio_cache.append(wav)\n",
    "            self.labels.append(row.label)\n",
    "\n",
    "        self.labels = torch.tensor(self.labels)\n",
    "\n",
    "        if errors > 0:\n",
    "            print(f\"锔 {errors} files failed to load (replaced with noise)\")\n",
    "        else:\n",
    "            print(\" All evaluation files cached in RAM\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_cache)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.audio_cache[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "#best_local_path = \"checkpoints/IR_ResNet_Triplet_combined_datasets_v2_2026-01-19_19-42-55/best-epoch=02-acc=0.9516.ckpt\"\n",
    "best_local_path = checkpoint_best.best_model_path\n",
    "print(\"path to best model:\", best_local_path)\n",
    "#best_local_path =\n",
    "\n",
    "Models_path = \"Models\"\n",
    "subfolder_name = \"Recall_at_K_results_two_datasets\"\n",
    "TARGET_RUN_DIR = os.path.join(Models_path, subfolder_name)\n",
    "\n",
    "\n",
    "model_pet_name = \"two_datasets_final_v1\" # np. V1, V2 itp.\n",
    "\n",
    "print(\"\\n ROZPOCZYNAM TESTOWANIE NAJLEPSZEGO MODELU (Recall@K)...\")\n",
    "\n",
    "\n",
    "ROOT_DIR = \"dataset/MAD_dataset\" \n",
    "\n",
    "test_csv_path = os.path.join(ROOT_DIR, \"test.csv\")\n",
    "df_test = pd.read_csv(test_csv_path)\n",
    "\n",
    "def fix_test_path(path):\n",
    "    path = str(path)\n",
    "    if not path.startswith(\"test/\") and not path.startswith(\"training/\"):\n",
    "        return os.path.join(\"test\", path)\n",
    "    return path\n",
    "\n",
    "df_test['path'] = df_test['path'].apply(fix_test_path)\n",
    "\n",
    "#  NEW: evaluation dataset (no triplets!)\n",
    "test_ds = CachedEvalAudioDataset(df_test, root_dir=ROOT_DIR)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_model = ResNetTripletGPU.load_from_checkpoint(\n",
    "    best_local_path,\n",
    "    df=df_full,\n",
    "    root_dir=ROOT_DIR,\n",
    "    noise_files=[]\n",
    ")\n",
    "best_model.eval()\n",
    "best_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# Embedding extraction\n",
    "# ==========================================\n",
    "print(\" Ekstrakcja embedding贸w...\")\n",
    "\n",
    "device = best_model.device\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for wav, lbl in test_loader:\n",
    "        wav = wav.to(device)\n",
    "        emb = best_model(wav)\n",
    "        embeddings.append(emb.cpu())\n",
    "        labels.append(lbl)\n",
    "\n",
    "embeddings = torch.cat(embeddings)   # [N, D]\n",
    "labels = torch.cat(labels)           # [N]\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "time_taken = (end_time - start_time).total_seconds() * 1000\n",
    "avg_inference_time = time_taken / len(test_ds)\n",
    "\n",
    "print(f\"憋 Czas testowania: {time_taken:.2f} ms\")\n",
    "print(f\"憋 redni czas inferencji na pr贸bk: {avg_inference_time:.4f} ms\")\n",
    "\n",
    "# ==========================================\n",
    "# Recall@K computation\n",
    "# ==========================================\n",
    "def recall_at_k(embeddings, labels, k=1):\n",
    "    emb = F.normalize(embeddings, dim=1)\n",
    "    sim = emb @ emb.T  # cosine similarity\n",
    "    sim.fill_diagonal_(-1)\n",
    "\n",
    "    topk = sim.topk(k, dim=1).indices\n",
    "    retrieved_labels = labels[topk]\n",
    "\n",
    "    correct = (retrieved_labels == labels.unsqueeze(1)).any(dim=1)\n",
    "    return correct.float().mean().item()\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def knn_f1(embeddings, labels, k=5):\n",
    "    emb = F.normalize(embeddings, dim=1)\n",
    "    sim = emb @ emb.T\n",
    "    sim.fill_diagonal_(-1)\n",
    "\n",
    "    knn = sim.topk(k, dim=1).indices\n",
    "    preds = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        neighbor_labels = labels[knn[i]]\n",
    "        pred = torch.mode(neighbor_labels).values\n",
    "        preds.append(pred.item())\n",
    "\n",
    "    return f1_score(labels.numpy(), preds, average=\"macro\")\n",
    "\n",
    "recall_1 = recall_at_k(embeddings, labels, k=1)\n",
    "recall_2 = recall_at_k(embeddings, labels, k=2)\n",
    "recall_5 = recall_at_k(embeddings, labels, k=5)\n",
    "recall_10 = recall_at_k(embeddings, labels, k=10)\n",
    "\n",
    "knn_f1_score = knn_f1(embeddings, labels, k=5)\n",
    "\n",
    "print(\"\\n WYNIKI BEST:\")\n",
    "print(f\"Recall@1  = {recall_1:.4f}\")\n",
    "print(f\"Recall@2  = {recall_2:.4f}\")\n",
    "print(f\"Recall@5  = {recall_5:.4f}\")\n",
    "print(f\"Recall@10 = {recall_10:.4f}\")\n",
    "print(f\"KNN F1 (k=5) = {knn_f1_score:.4f}\")\n",
    "\n",
    "\n",
    "# Upewnij si, 偶e katalog istnieje\n",
    "os.makedirs(TARGET_RUN_DIR, exist_ok=True)\n",
    "\n",
    "best_filename = (\n",
    "    f\"{model_pet_name}\"\n",
    "    f\"_R1={recall_1:.4f}\"\n",
    "    f\"_R5={recall_5:.4f}\"\n",
    "    f\"_R10={recall_10:.4f}\"\n",
    "    f\"_time={avg_inference_time:.4f}.ckpt\"\n",
    ")\n",
    "\n",
    "target_best = os.path.join(TARGET_RUN_DIR, best_filename)\n",
    "\n",
    "try:\n",
    "    shutil.copy(best_local_path, target_best)\n",
    "    print(f\" Zapisano NAJLEPSZY model: {best_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\" Bd kopiowania BEST: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
